{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://LAPTOP-D4GN0MQF:4042\n",
       "SparkContext available as 'sc' (version = 2.3.0, master = local[*], app id = local-1573069627261)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n"
     ]
    }
   ],
   "source": [
    "println(\"Start\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Sum consecutive value by Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.expressions.Window\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.expressions.Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+---------+\n",
      "|Category|Value|Sequences|\n",
      "+--------+-----+---------+\n",
      "|       1|   10|        0|\n",
      "|       1|   11|        1|\n",
      "|       1|   13|        1|\n",
      "|       1|   16|        1|\n",
      "|       1|   20|        0|\n",
      "|       1|   21|        0|\n",
      "|       1|   22|        1|\n",
      "|       1|   25|        1|\n",
      "|       1|   27|        1|\n",
      "|       1|   29|        1|\n",
      "|       1|   30|        0|\n",
      "|       1|   32|        1|\n",
      "|       1|   34|        1|\n",
      "|       1|   35|        1|\n",
      "|       1|   38|        0|\n",
      "+--------+-----+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [Category: int, Value: int ... 1 more field]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = Seq(\n",
    "  (1, 10, 0),\n",
    "  (1, 11, 1),\n",
    "  (1, 13, 1),\n",
    "  (1, 16, 1),\n",
    "  (1, 20, 0),\n",
    "  (1, 21, 0),\n",
    "  (1, 22, 1),\n",
    "  (1, 25, 1),\n",
    "  (1, 27, 1),\n",
    "  (1, 29, 1),\n",
    "  (1, 30, 0),\n",
    "  (1, 32, 1),\n",
    "  (1, 34, 1),\n",
    "  (1, 35, 1),\n",
    "  (1, 38, 0)).toDF(\"Category\", \"Value\", \"Sequences\")\n",
    "df.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "assing each row unique id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+---------+---+\n",
      "|Category|Value|Sequences|zip|\n",
      "+--------+-----+---------+---+\n",
      "|       1|   10|        0|  0|\n",
      "|       1|   11|        1|  1|\n",
      "|       1|   13|        1|  2|\n",
      "|       1|   16|        1|  3|\n",
      "|       1|   20|        0|  4|\n",
      "|       1|   21|        0|  5|\n",
      "|       1|   22|        1|  6|\n",
      "|       1|   25|        1|  7|\n",
      "|       1|   27|        1|  8|\n",
      "|       1|   29|        1|  9|\n",
      "|       1|   30|        0| 10|\n",
      "|       1|   32|        1| 11|\n",
      "|       1|   34|        1| 12|\n",
      "|       1|   35|        1| 13|\n",
      "|       1|   38|        0| 14|\n",
      "+--------+-----+---------+---+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "zipped: org.apache.spark.sql.DataFrame = [Category: int, Value: int ... 2 more fields]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val zipped = df.withColumn(\"zip\", monotonically_increasing_id())\n",
    "zipped.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make range from zero to the next zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+---------+----------+--------+\n",
      "|Category|Value|Sequences|rangeStart|rangeEnd|\n",
      "+--------+-----+---------+----------+--------+\n",
      "|1       |10   |0        |0         |4       |\n",
      "|1       |20   |0        |4         |5       |\n",
      "|1       |21   |0        |5         |10      |\n",
      "|1       |30   |0        |10        |14      |\n",
      "|1       |38   |0        |14        |null    |\n",
      "+--------+-----+---------+----------+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "categoryWindow: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@665afaa9\r\n",
       "groups: org.apache.spark.sql.DataFrame = [Category: int, Value: int ... 3 more fields]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val categoryWindow = Window.partitionBy(\"Category\").orderBy($\"zip\")\n",
    "val groups = zipped\n",
    "             .filter($\"Sequences\" === 0)\n",
    "             .withColumn(\"rangeEnd\", lead($\"zip\",1).over(categoryWindow))\n",
    "             .withColumnRenamed(\"zip\", \"rangeStart\")\n",
    "groups.show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "assing range for each unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "joinCondition: org.apache.spark.sql.Column = ((units.zip > groups.rangeStart) AND (units.zip < groups.rangeEnd))\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val joinCondition = ($\"units.zip\" > $\"groups.rangeStart\").and($\"units.zip\" < $\"groups.rangeEnd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unitsByRange: org.apache.spark.sql.DataFrame = [Category: int, Value: int ... 1 more field]\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val unitsByRange = zipped\n",
    "                    .filter($\"Sequences\" === 1).alias(\"units\")\n",
    "                    .join(groups.alias(\"groups\"), joinCondition, \"left\")\n",
    "                    .select(\"units.Category\", \"units.Value\", \"groups.rangeStart\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+----------+\n",
      "|Category|Value|rangeStart|\n",
      "+--------+-----+----------+\n",
      "|       1|   11|         0|\n",
      "|       1|   13|         0|\n",
      "|       1|   16|         0|\n",
      "|       1|   22|         5|\n",
      "|       1|   25|         5|\n",
      "|       1|   27|         5|\n",
      "|       1|   29|         5|\n",
      "|       1|   32|        10|\n",
      "|       1|   34|        10|\n",
      "|       1|   35|        10|\n",
      "+--------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unitsByRange.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "group by range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "result: org.apache.spark.sql.DataFrame = [Category: int, summing: bigint]\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val result = unitsByRange\n",
    "                .groupBy($\"Category\", $\"rangeStart\")\n",
    "                .agg(sum(\"Value\").alias(\"summing\"))\n",
    "                .orderBy(\"rangeStart\")\n",
    "                .drop(\"rangeStart\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+\n",
      "|Category|summing|\n",
      "+--------+-------+\n",
      "|1       |40     |\n",
      "|1       |103    |\n",
      "|1       |101    |\n",
      "+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Group By (And sum) consecutive Time Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions\r\n",
       "import org.apache.spark.sql.expressions.Window\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions\n",
    "import org.apache.spark.sql.expressions.Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+\n",
      "|signal|          timestamp|\n",
      "+------+-------------------+\n",
      "|     0|2016-07-02 12:01:40|\n",
      "|     1|2016-07-02 12:21:23|\n",
      "|     1|2016-07-02 13:22:56|\n",
      "|     1|2016-07-02 13:27:07|\n",
      "|     0|2016-07-02 13:30:12|\n",
      "|     0|2016-07-02 13:40:34|\n",
      "|     1|2016-07-02 13:57:07|\n",
      "|     1|2016-07-02 14:08:07|\n",
      "+------+-------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [signal: int, timestamp: timestamp]\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = Seq((0, \"2016-07-02 12:01:40\"),\n",
    " (1, \"2016-07-02 12:21:23\"),\n",
    " (1, \"2016-07-02 13:22:56\"),\n",
    " (1, \"2016-07-02 13:27:07\"),\n",
    " (0, \"2016-07-02 13:30:12\"),\n",
    " (0, \"2016-07-02 13:40:34\"),\n",
    " (1, \"2016-07-02 13:57:07\"),\n",
    " (1, \"2016-07-02 14:08:07\")).\n",
    "toDF(\"signal\", \"timestamp\").\n",
    "withColumn(\"timestamp\", functions.to_timestamp(col(\"timestamp\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "newSignalWindow: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@20513eb3\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val newSignalWindow = Window.orderBy(\"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfWithNewSignal: org.apache.spark.sql.DataFrame = [signal: int, timestamp: timestamp ... 1 more field]\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfWithNewSignal = df.withColumn(\"new_signal\",\n",
    "                                    (functions.lag(col(\"signal\"),1,0).over(newSignalWindow) === 0 && \n",
    "                                     col(\"signal\") === 1).cast(\"bigint\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-06 16:47:27 WARN  WindowExec:66 - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "+------+-------------------+----------+\n",
      "|signal|          timestamp|new_signal|\n",
      "+------+-------------------+----------+\n",
      "|     0|2016-07-02 12:01:40|         0|\n",
      "|     1|2016-07-02 12:21:23|         1|\n",
      "|     1|2016-07-02 13:22:56|         0|\n",
      "|     1|2016-07-02 13:27:07|         0|\n",
      "|     0|2016-07-02 13:30:12|         0|\n",
      "|     0|2016-07-02 13:40:34|         0|\n",
      "|     1|2016-07-02 13:57:07|         1|\n",
      "|     1|2016-07-02 14:08:07|         0|\n",
      "+------+-------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfWithNewSignal.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfWithIdSignal: org.apache.spark.sql.DataFrame = [signal: int, timestamp: timestamp ... 1 more field]\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfWithIdSignal = dfWithNewSignal.\n",
    "                            filter(col(\"signal\") === 1).\n",
    "                            withColumn(\"new_signal\", \n",
    "                                       functions.\n",
    "                                          sum(\"new_signal\").\n",
    "                                          over(newSignalWindow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-06 16:47:29 WARN  WindowExec:66 - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "2019-11-06 16:47:29 WARN  WindowExec:66 - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "+------+-------------------+----------+\n",
      "|signal|          timestamp|new_signal|\n",
      "+------+-------------------+----------+\n",
      "|     1|2016-07-02 12:21:23|         1|\n",
      "|     1|2016-07-02 13:22:56|         1|\n",
      "|     1|2016-07-02 13:27:07|         1|\n",
      "|     1|2016-07-02 13:57:07|         2|\n",
      "|     1|2016-07-02 14:08:07|         2|\n",
      "+------+-------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfWithIdSignal.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "resultDF: org.apache.spark.sql.DataFrame = [start_date: timestamp, end_date: timestamp ... 1 more field]\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val resultDF = dfWithIdSignal.\n",
    "                  groupBy(\"new_signal\").\n",
    "                      agg(functions.min(\"timestamp\").as(\"start_date\"), functions.max(\"timestamp\").as(\"end_date\"),\n",
    "                          functions.count(\"*\").as(\"positive_count\")).drop(\"new_signal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-06 16:47:29 WARN  WindowExec:66 - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "2019-11-06 16:47:29 WARN  WindowExec:66 - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "+-------------------+-------------------+--------------+\n",
      "|         start_date|           end_date|positive_count|\n",
      "+-------------------+-------------------+--------------+\n",
      "|2016-07-02 12:21:23|2016-07-02 13:27:07|             3|\n",
      "|2016-07-02 13:57:07|2016-07-02 14:08:07|             2|\n",
      "+-------------------+-------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resultDF.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Count Occurence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [id: int, date: string ... 1 more field]\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = Seq(\n",
    "  (1, \"4/8/2019\", \"CLM***120379893***John***CLM***Smith***blablabla**so..on…\"),\n",
    "  (2, \"4/8/2019\", \"CLM***120379093***John***CLM***Smith***CLM***blablabla**so..on…\"),\n",
    "  (3, \"4/8/2019\", \"CLM***139979893***John***Smith***blablabla**so..on…\")\n",
    ").toDF(\"id\", \"date\", \"content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "countAll: (pattern: String)org.apache.spark.sql.expressions.UserDefinedFunction\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def countAll(pattern: String) = udf((s: String) => pattern.r.findAllIn(s).size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------------------+---------+\n",
      "| id|    date|             content|wordcount|\n",
      "+---+--------+--------------------+---------+\n",
      "|  1|4/8/2019|CLM***120379893**...|        2|\n",
      "|  2|4/8/2019|CLM***120379093**...|        3|\n",
      "|  3|4/8/2019|CLM***139979893**...|        1|\n",
      "+---+--------+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"wordcount\", countAll(\"CLM\")($\"content\")).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Check if string exist in a column of another dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+\n",
      "| id|              url|\n",
      "+---+-----------------+\n",
      "|  1|       google.com|\n",
      "|  2|     facebook.com|\n",
      "|  3|       github.com|\n",
      "|  4|stackoverflow.com|\n",
      "+---+-----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df1: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: int, url: string]\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  val df1 = Seq(\n",
    "    (1, \"google.com\"),\n",
    "    (2, \"facebook.com\"),\n",
    "    (3, \"github.com\"),\n",
    "    (4, \"stackoverflow.com\")).toDF(\"id\", \"url\").as(\"first\")\n",
    "  df1.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+\n",
      "| id|         url|\n",
      "+---+------------+\n",
      "| 11|  google.com|\n",
      "| 12|   yahoo.com|\n",
      "| 13|facebook.com|\n",
      "| 14| twitter.com|\n",
      "+---+------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df2: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: int, url: string]\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  val df2 = Seq(\n",
    "    (11, \"google.com\"),\n",
    "    (12, \"yahoo.com\"),\n",
    "    (13, \"facebook.com\"),\n",
    "    (14, \"twitter.com\")).toDF(\"id\", \"url\").as(\"second\")\n",
    "  df2.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import spark.implicits._\r\n",
       "import org.apache.spark.sql.functions.{col, _}\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._\n",
    "import org.apache.spark.sql.functions.{col, _}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df3: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [url: string, check: boolean]\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df3 = df2.join(df1, expr(\"first.url like second.url\"), \"full_outer\")\n",
    "                        .select(col(\"first.url\"),col(\"first.url\")\n",
    "                                .contains(col(\"second.url\")).as(\"check\"))\n",
    "                                    .filter(\"url is not null\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|              url|check|\n",
      "+-----------------+-----+\n",
      "|       google.com| true|\n",
      "|     facebook.com| true|\n",
      "|       github.com| null|\n",
      "|stackoverflow.com| null|\n",
      "+-----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|              url|check|\n",
      "+-----------------+-----+\n",
      "|       google.com| true|\n",
      "|     facebook.com| true|\n",
      "|       github.com|false|\n",
      "|stackoverflow.com|false|\n",
      "+-----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.na.fill(Map(\"check\" -> false)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - Spark Higher Order to compute Top N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql._\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " import org.apache.spark.sql._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [id: string, product_list: string]\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  val df = Seq(\n",
    "      (\"1\", \"p1,p1,p1,p1,p1,p3,p3,p2,p2,p2,p2\"),\n",
    "      (\"2\", \"p2,p2,p2,p2,p2,p4,p4,p4,p1,p3\")\n",
    "    ).toDF(\"id\", \"product_list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|        product_list|\n",
      "+---+--------------------+\n",
      "|  1|p1,p1,p1,p1,p1,p3...|\n",
      "|  2|p2,p2,p2,p2,p2,p4...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "getMetrics: (value: org.apache.spark.sql.Row, n: Int)(String, String)\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getMetrics(value: Row, n: Int): (String, String) = {\n",
    "    \n",
    "    val split = value.getAs[String](\"product_list\").split(\",\")\n",
    "    \n",
    "    val sortedRecords = split.groupBy(x => x).map(data => (data._1, data._2.size)).toList.sortWith(_._2 > _._2)\n",
    "    (value.getAs[String](\"id\"), sortedRecords.take(n).map(_._1).mkString(\",\"))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "final_df: org.apache.spark.sql.DataFrame = [id: string, most_seen_products: string]\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val final_df = df.map(value =>\n",
    "                     getMetrics(value,2)\n",
    "                      ).withColumnRenamed(\"_1\", \"id\")\n",
    "                       .withColumnRenamed(\"_2\", \"most_seen_products\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "| id|most_seen_products|\n",
      "+---+------------------+\n",
      "|  1|             p1,p2|\n",
      "|  2|             p2,p4|\n",
      "+---+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
