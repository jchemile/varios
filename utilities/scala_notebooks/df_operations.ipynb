{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://LAPTOP-D4GN0MQF:4040\n",
       "SparkContext available as 'sc' (version = 2.3.0, master = local[*], app id = local-1572277513469)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-28 12:45:11 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Start\n"
     ]
    }
   ],
   "source": [
    "println(\"Start\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Sum consecutive value by Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.expressions.Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df = Seq(\n",
    "  (1, 10, 0),\n",
    "  (1, 11, 1),\n",
    "  (1, 13, 1),\n",
    "  (1, 16, 1),\n",
    "  (1, 20, 0),\n",
    "  (1, 21, 0),\n",
    "  (1, 22, 1),\n",
    "  (1, 25, 1),\n",
    "  (1, 27, 1),\n",
    "  (1, 29, 1),\n",
    "  (1, 30, 0),\n",
    "  (1, 32, 1),\n",
    "  (1, 34, 1),\n",
    "  (1, 35, 1),\n",
    "  (1, 38, 0)).toDF(\"Category\", \"Value\", \"Sequences\")\n",
    "df.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "assing each row unique id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val zipped = df.withColumn(\"zip\", monotonically_increasing_id())\n",
    "zipped.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make range from zero to the next zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val categoryWindow = Window.partitionBy(\"Category\").orderBy($\"zip\")\n",
    "val groups = zipped\n",
    "             .filter($\"Sequences\" === 0)\n",
    "             .withColumn(\"rangeEnd\", lead($\"zip\",1).over(categoryWindow))\n",
    "             .withColumnRenamed(\"zip\", \"rangeStart\")\n",
    "groups.show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "assing range for each unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val joinCondition = ($\"units.zip\" > $\"groups.rangeStart\").and($\"units.zip\" < $\"groups.rangeEnd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val unitsByRange = zipped\n",
    "                    .filter($\"Sequences\" === 1).alias(\"units\")\n",
    "                    .join(groups.alias(\"groups\"), joinCondition, \"left\")\n",
    "                    .select(\"units.Category\", \"units.Value\", \"groups.rangeStart\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unitsByRange.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "group by range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val result = unitsByRange\n",
    "                .groupBy($\"Category\", $\"rangeStart\")\n",
    "                .agg(sum(\"Value\").alias(\"summing\"))\n",
    "                .orderBy(\"rangeStart\")\n",
    "                .drop(\"rangeStart\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Group By (And sum) consecutive Time Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions\n",
    "import org.apache.spark.sql.expressions.Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df = Seq((0, \"2016-07-02 12:01:40\"),\n",
    " (1, \"2016-07-02 12:21:23\"),\n",
    " (1, \"2016-07-02 13:22:56\"),\n",
    " (1, \"2016-07-02 13:27:07\"),\n",
    " (0, \"2016-07-02 13:30:12\"),\n",
    " (0, \"2016-07-02 13:40:34\"),\n",
    " (1, \"2016-07-02 13:57:07\"),\n",
    " (1, \"2016-07-02 14:08:07\")).\n",
    "toDF(\"signal\", \"timestamp\").\n",
    "withColumn(\"timestamp\", functions.to_timestamp(col(\"timestamp\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val newSignalWindow = Window.orderBy(\"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val dfWithNewSignal = df.withColumn(\"new_signal\",\n",
    "                                    (functions.lag(col(\"signal\"),1,0).over(newSignalWindow) === 0 && \n",
    "                                     col(\"signal\") === 1).cast(\"bigint\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfWithNewSignal.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val dfWithIdSignal = dfWithNewSignal.\n",
    "                            filter(col(\"signal\") === 1).\n",
    "                            withColumn(\"new_signal\", \n",
    "                                       functions.\n",
    "                                          sum(\"new_signal\").\n",
    "                                          over(newSignalWindow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfWithIdSignal.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val resultDF = dfWithIdSignal.\n",
    "                  groupBy(\"new_signal\").\n",
    "                      agg(functions.min(\"timestamp\").as(\"start_date\"), functions.max(\"timestamp\").as(\"end_date\"),\n",
    "                          functions.count(\"*\").as(\"positive_count\")).drop(\"new_signal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultDF.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Count Occurence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df = Seq(\n",
    "  (1, \"4/8/2019\", \"CLM***120379893***John***CLM***Smith***blablabla**so..on…\"),\n",
    "  (2, \"4/8/2019\", \"CLM***120379093***John***CLM***Smith***CLM***blablabla**so..on…\"),\n",
    "  (3, \"4/8/2019\", \"CLM***139979893***John***Smith***blablabla**so..on…\")\n",
    ").toDF(\"id\", \"date\", \"content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countAll(pattern: String) = udf((s: String) => pattern.r.findAllIn(s).size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn(\"wordcount\", countAll(\"CLM\")($\"content\")).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Check if string exist in a column of another dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  val df1 = Seq(\n",
    "    (1, \"google.com\"),\n",
    "    (2, \"facebook.com\"),\n",
    "    (3, \"github.com\"),\n",
    "    (4, \"stackoverflow.com\")).toDF(\"id\", \"url\").as(\"first\")\n",
    "  df1.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  val df2 = Seq(\n",
    "    (11, \"google.com\"),\n",
    "    (12, \"yahoo.com\"),\n",
    "    (13, \"facebook.com\"),\n",
    "    (14, \"twitter.com\")).toDF(\"id\", \"url\").as(\"second\")\n",
    "  df2.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spark.implicits._\n",
    "import org.apache.spark.sql.functions.{col, _}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df3 = df2.join(df1, expr(\"first.url like second.url\"), \"full_outer\")\n",
    "                        .select(col(\"first.url\"),col(\"first.url\")\n",
    "                                .contains(col(\"second.url\")).as(\"check\"))\n",
    "                                    .filter(\"url is not null\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.na.fill(Map(\"check\" -> false)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - Spark Higher Order to compute Top N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql._\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " import org.apache.spark.sql._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [id: string, product_list: string]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  val df = Seq(\n",
    "      (\"1\", \"p1,p1,p1,p1,p1,p3,p3,p2,p2,p2,p2\"),\n",
    "      (\"2\", \"p2,p2,p2,p2,p2,p4,p4,p4,p1,p3\")\n",
    "    ).toDF(\"id\", \"product_list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|        product_list|\n",
      "+---+--------------------+\n",
      "|  1|p1,p1,p1,p1,p1,p3...|\n",
      "|  2|p2,p2,p2,p2,p2,p4...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "getMetrics: (value: org.apache.spark.sql.Row, n: Int)(String, String)\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getMetrics(value: Row, n: Int): (String, String) = {\n",
    "    \n",
    "    val split = value.getAs[String](\"product_list\").split(\",\")\n",
    "    \n",
    "    val sortedRecords = split.groupBy(x => x).map(data => (data._1, data._2.size)).toList.sortWith(_._2 > _._2)\n",
    "    (value.getAs[String](\"id\"), sortedRecords.take(n).map(_._1).mkString(\",\"))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "final_df: org.apache.spark.sql.DataFrame = [id: string, most_seen_products: string]\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val final_df = df.map(value =>\n",
    "                     getMetrics(value,2)\n",
    "                      ).withColumnRenamed(\"_1\", \"id\")\n",
    "                       .withColumnRenamed(\"_2\", \"most_seen_products\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "| id|most_seen_products|\n",
      "+---+------------------+\n",
      "|  1|             p1,p2|\n",
      "|  2|             p2,p4|\n",
      "+---+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
