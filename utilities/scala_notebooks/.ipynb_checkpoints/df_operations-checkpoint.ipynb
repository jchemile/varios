{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n"
     ]
    }
   ],
   "source": [
    "println(\"Start\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Sum consecutive value by Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.expressions.Window\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.expressions.Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+---------+\n",
      "|Category|Value|Sequences|\n",
      "+--------+-----+---------+\n",
      "|       1|   10|        0|\n",
      "|       1|   11|        1|\n",
      "|       1|   13|        1|\n",
      "|       1|   16|        1|\n",
      "|       1|   20|        0|\n",
      "|       1|   21|        0|\n",
      "|       1|   22|        1|\n",
      "|       1|   25|        1|\n",
      "|       1|   27|        1|\n",
      "|       1|   29|        1|\n",
      "|       1|   30|        0|\n",
      "|       1|   32|        1|\n",
      "|       1|   34|        1|\n",
      "|       1|   35|        1|\n",
      "|       1|   38|        0|\n",
      "+--------+-----+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [Category: int, Value: int ... 1 more field]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = Seq(\n",
    "  (1, 10, 0),\n",
    "  (1, 11, 1),\n",
    "  (1, 13, 1),\n",
    "  (1, 16, 1),\n",
    "  (1, 20, 0),\n",
    "  (1, 21, 0),\n",
    "  (1, 22, 1),\n",
    "  (1, 25, 1),\n",
    "  (1, 27, 1),\n",
    "  (1, 29, 1),\n",
    "  (1, 30, 0),\n",
    "  (1, 32, 1),\n",
    "  (1, 34, 1),\n",
    "  (1, 35, 1),\n",
    "  (1, 38, 0)).toDF(\"Category\", \"Value\", \"Sequences\")\n",
    "df.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "assing each row unique id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+---------+---+\n",
      "|Category|Value|Sequences|zip|\n",
      "+--------+-----+---------+---+\n",
      "|       1|   10|        0|  0|\n",
      "|       1|   11|        1|  1|\n",
      "|       1|   13|        1|  2|\n",
      "|       1|   16|        1|  3|\n",
      "|       1|   20|        0|  4|\n",
      "|       1|   21|        0|  5|\n",
      "|       1|   22|        1|  6|\n",
      "|       1|   25|        1|  7|\n",
      "|       1|   27|        1|  8|\n",
      "|       1|   29|        1|  9|\n",
      "|       1|   30|        0| 10|\n",
      "|       1|   32|        1| 11|\n",
      "|       1|   34|        1| 12|\n",
      "|       1|   35|        1| 13|\n",
      "|       1|   38|        0| 14|\n",
      "+--------+-----+---------+---+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "zipped: org.apache.spark.sql.DataFrame = [Category: int, Value: int ... 2 more fields]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val zipped = df.withColumn(\"zip\", monotonically_increasing_id())\n",
    "zipped.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make range from zero to the next zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+---------+----------+--------+\n",
      "|Category|Value|Sequences|rangeStart|rangeEnd|\n",
      "+--------+-----+---------+----------+--------+\n",
      "|1       |10   |0        |0         |4       |\n",
      "|1       |20   |0        |4         |5       |\n",
      "|1       |21   |0        |5         |10      |\n",
      "|1       |30   |0        |10        |14      |\n",
      "|1       |38   |0        |14        |null    |\n",
      "+--------+-----+---------+----------+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "categoryWindow: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@5f58cd28\r\n",
       "groups: org.apache.spark.sql.DataFrame = [Category: int, Value: int ... 3 more fields]\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val categoryWindow = Window.partitionBy(\"Category\").orderBy($\"zip\")\n",
    "val groups = zipped\n",
    "             .filter($\"Sequences\" === 0)\n",
    "             .withColumn(\"rangeEnd\", lead($\"zip\",1).over(categoryWindow))\n",
    "             .withColumnRenamed(\"zip\", \"rangeStart\")\n",
    "groups.show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "assing range for each unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "joinCondition: org.apache.spark.sql.Column = ((units.zip > groups.rangeStart) AND (units.zip < groups.rangeEnd))\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val joinCondition = ($\"units.zip\" > $\"groups.rangeStart\").and($\"units.zip\" < $\"groups.rangeEnd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unitsByRange: org.apache.spark.sql.DataFrame = [Category: int, Value: int ... 1 more field]\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val unitsByRange = zipped\n",
    "                    .filter($\"Sequences\" === 1).alias(\"units\")\n",
    "                    .join(groups.alias(\"groups\"), joinCondition, \"left\")\n",
    "                    .select(\"units.Category\", \"units.Value\", \"groups.rangeStart\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+----------+\n",
      "|Category|Value|rangeStart|\n",
      "+--------+-----+----------+\n",
      "|       1|   11|         0|\n",
      "|       1|   13|         0|\n",
      "|       1|   16|         0|\n",
      "|       1|   22|         5|\n",
      "|       1|   25|         5|\n",
      "|       1|   27|         5|\n",
      "|       1|   29|         5|\n",
      "|       1|   32|        10|\n",
      "|       1|   34|        10|\n",
      "|       1|   35|        10|\n",
      "+--------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unitsByRange.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "group by range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "result: org.apache.spark.sql.DataFrame = [Category: int, summing: bigint]\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val result = unitsByRange\n",
    "                .groupBy($\"Category\", $\"rangeStart\")\n",
    "                .agg(sum(\"Value\").alias(\"summing\"))\n",
    "                .orderBy(\"rangeStart\")\n",
    "                .drop(\"rangeStart\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+\n",
      "|Category|summing|\n",
      "+--------+-------+\n",
      "|1       |40     |\n",
      "|1       |103    |\n",
      "|1       |101    |\n",
      "+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Group By (And sum) consecutive Time Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions\r\n",
       "import org.apache.spark.sql.expressions.Window\n"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions\n",
    "import org.apache.spark.sql.expressions.Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+\n",
      "|signal|          timestamp|\n",
      "+------+-------------------+\n",
      "|     0|2016-07-02 12:01:40|\n",
      "|     1|2016-07-02 12:21:23|\n",
      "|     1|2016-07-02 13:22:56|\n",
      "|     1|2016-07-02 13:27:07|\n",
      "|     0|2016-07-02 13:30:12|\n",
      "|     0|2016-07-02 13:40:34|\n",
      "|     1|2016-07-02 13:57:07|\n",
      "|     1|2016-07-02 14:08:07|\n",
      "+------+-------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [signal: int, timestamp: timestamp]\n"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = Seq((0, \"2016-07-02 12:01:40\"),\n",
    " (1, \"2016-07-02 12:21:23\"),\n",
    " (1, \"2016-07-02 13:22:56\"),\n",
    " (1, \"2016-07-02 13:27:07\"),\n",
    " (0, \"2016-07-02 13:30:12\"),\n",
    " (0, \"2016-07-02 13:40:34\"),\n",
    " (1, \"2016-07-02 13:57:07\"),\n",
    " (1, \"2016-07-02 14:08:07\")).\n",
    "toDF(\"signal\", \"timestamp\").\n",
    "withColumn(\"timestamp\", functions.to_timestamp(col(\"timestamp\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "newSignalWindow: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@3c4ac36b\n"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val newSignalWindow = Window.orderBy(\"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfWithNewSignal: org.apache.spark.sql.DataFrame = [signal: int, timestamp: timestamp ... 1 more field]\n"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfWithNewSignal = df.withColumn(\"new_signal\",\n",
    "                                    (functions.lag(col(\"signal\"),1,0).over(newSignalWindow) === 0 && \n",
    "                                     col(\"signal\") === 1).cast(\"bigint\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-21 11:47:04 WARN  WindowExec:66 - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "+------+-------------------+----------+\n",
      "|signal|          timestamp|new_signal|\n",
      "+------+-------------------+----------+\n",
      "|     0|2016-07-02 12:01:40|         0|\n",
      "|     1|2016-07-02 12:21:23|         1|\n",
      "|     1|2016-07-02 13:22:56|         0|\n",
      "|     1|2016-07-02 13:27:07|         0|\n",
      "|     0|2016-07-02 13:30:12|         0|\n",
      "|     0|2016-07-02 13:40:34|         0|\n",
      "|     1|2016-07-02 13:57:07|         1|\n",
      "|     1|2016-07-02 14:08:07|         0|\n",
      "+------+-------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfWithNewSignal.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfWithIdSignal: org.apache.spark.sql.DataFrame = [signal: int, timestamp: timestamp ... 1 more field]\n"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfWithIdSignal = dfWithNewSignal.\n",
    "                            filter(col(\"signal\") === 1).\n",
    "                            withColumn(\"new_signal\", \n",
    "                                       functions.\n",
    "                                          sum(\"new_signal\").\n",
    "                                          over(newSignalWindow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-21 11:52:54 WARN  WindowExec:66 - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "2019-08-21 11:52:54 WARN  WindowExec:66 - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "+------+-------------------+----------+\n",
      "|signal|          timestamp|new_signal|\n",
      "+------+-------------------+----------+\n",
      "|     1|2016-07-02 12:21:23|         1|\n",
      "|     1|2016-07-02 13:22:56|         1|\n",
      "|     1|2016-07-02 13:27:07|         1|\n",
      "|     1|2016-07-02 13:57:07|         2|\n",
      "|     1|2016-07-02 14:08:07|         2|\n",
      "+------+-------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfWithIdSignal.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "resultDF: org.apache.spark.sql.DataFrame = [start_date: timestamp, end_date: timestamp ... 1 more field]\n"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val resultDF = dfWithIdSignal.\n",
    "                  groupBy(\"new_signal\").\n",
    "                      agg(functions.min(\"timestamp\").as(\"start_date\"), functions.max(\"timestamp\").as(\"end_date\"),\n",
    "                          functions.count(\"*\").as(\"positive_count\")).drop(\"new_signal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-21 11:56:47 WARN  WindowExec:66 - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "2019-08-21 11:56:47 WARN  WindowExec:66 - No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "+-------------------+-------------------+--------------+\n",
      "|         start_date|           end_date|positive_count|\n",
      "+-------------------+-------------------+--------------+\n",
      "|2016-07-02 12:21:23|2016-07-02 13:27:07|             3|\n",
      "|2016-07-02 13:57:07|2016-07-02 14:08:07|             2|\n",
      "+-------------------+-------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resultDF.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
