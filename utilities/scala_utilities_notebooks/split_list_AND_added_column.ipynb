{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://DESKTOP-AJ0MJTE:4040\n",
       "SparkContext available as 'sc' (version = 2.4.2, master = local[*], app id = local-1557088926225)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n"
     ]
    }
   ],
   "source": [
    "println(\"Start\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split a List to be added to a Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map[String, List[(Int, Int, Int)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "line: String = Mon-18-June-2018,1:10:5,2:20:10,3:30:15,4:40:20,5:50:25\r\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val line = \"Mon-18-June-2018,1:10:5,2:20:10,3:30:15,4:40:20,5:50:25\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "arr: Array[String] = Array(Mon-18-June-2018, 1:10:5, 2:20:10, 3:30:15, 4:40:20, 5:50:25)\r\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val arr = line.split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "map: scala.collection.mutable.Map[String,List[List[Int]]] = Map()\r\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val map = scala.collection.mutable.Map[String,List[List[Int]]]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "key: String = Mon-18-June-2018\r\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val key = arr(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "values: List[List[Int]] = List(List(1, 10, 5), List(2, 20, 10), List(3, 30, 15), List(4, 40, 20), List(5, 50, 25))\r\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val values = arr.toList.drop(1).map{\n",
    "    case str: String => \n",
    "        str.split(\":\").map(_.toInt).foldLeft(List[Int]())(\n",
    "            (acc,res) => \n",
    "                acc :+ res\n",
    "        )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res4: map.type = Map(Mon-18-June-2018 -> List(List(1, 10, 5), List(2, 20, 10), List(3, 30, 15), List(4, 40, 20), List(5, 50, 25)))\r\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map += (key -> values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Append multiple columns to existing data frame in spar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: org.apache.spark.sql.DataFrame = [_1: string, _2: int]\r\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = Seq((\"One\", 1),(\"Two\", 2),(\"three\", 3),(\"four\", 4) ).toDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "columnMap: scala.collection.immutable.Map[String,String] = Map(col1 -> val1, col2 -> val2)\r\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val columnMap = Map(\"col1\" -> \"val1\", \"col2\" -> \"val2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "newCols: Iterable[org.apache.spark.sql.Column] = Set(val1 AS `col1`, val2 AS `col2`)\r\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val newCols = columnMap.keys.map(k => lit(columnMap(k)) as k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----+----+\n",
      "|   _1| _2|col1|col2|\n",
      "+-----+---+----+----+\n",
      "|  One|  1|val1|val2|\n",
      "|  Two|  2|val1|val2|\n",
      "|three|  3|val1|val2|\n",
      "| four|  4|val1|val2|\n",
      "+-----+---+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(data.columns.map(col) ++newCols: _*).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
